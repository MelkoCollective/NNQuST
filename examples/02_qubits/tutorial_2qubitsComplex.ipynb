{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IN DEVELOPMENT\n",
    "\n",
    "# Part 2: Training an RBM *with* a phase\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "The following imports are needed to run this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from rbm_tutorial import RBM_Module, BinomialRBM\n",
    "import torch\n",
    "#from observables_tutorial import TFIMChainEnergy, TFIMChainMagnetization\n",
    "import numpy as np\n",
    "import csv\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('../../qucumber/')\n",
    "from complex_wavefunction import ComplexWavefunction\n",
    "from quantum_reconstruction import QuantumReconstruction\n",
    "sys.path.append('../../qucumber/utils/')\n",
    "import unitaries\n",
    "import utils.training_statistics as ts\n",
    "import pickle\n",
    "#import importlib.util\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = np.loadtxt('qubits_train_samples.txt', dtype= 'float32')\n",
    "target_psi  = torch.tensor(np.loadtxt('qubits_psi.txt', dtype= 'float32'), dtype=torch.double, device = torch.device('cpu'))\n",
    "train_bases = np.loadtxt('qubits_train_bases.txt',dtype=str)\n",
    "bases = np.loadtxt('qubits_bases.txt',dtype=str)\n",
    "unitary_dict = unitaries.create_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_samples = np.loadtxt('qubits_train_samples.txt', dtype= 'float32')\n",
    "target_psi  = torch.tensor(np.loadtxt('qubits_psi.txt', dtype= 'float32'), dtype=torch.double, device = torch.device('cpu'))\n",
    "train_bases = np.loadtxt('qubits_bases.txt',dtype=str)\n",
    "\n",
    "\n",
    "with open('../qucumber/tests/data_test.pkl', 'rb') as fin:\n",
    "    test_data = pickle.load(fin)\n",
    "\n",
    "train_bases = test_data['2qubits']['train_bases']\n",
    "train_samples = torch.tensor(test_data['2qubits']['train_samples'],dtype = torch.double)\n",
    "bases = test_data['2qubits']['bases']\n",
    "target_psi_dict=torch.tensor(test_data['2qubits']['target_psi'],dtype = torch.double)\n",
    "\n",
    "unitary_dict = unitaries.create_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nv = train_samples.shape[-1]\n",
    "nh = nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_state = ComplexWavefunction(num_visible=nv,num_hidden=nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs     = 200\n",
    "num_chains = 10\n",
    "batch_size = 10\n",
    "k          = 10\n",
    "lr         = 0.1\n",
    "log_every  = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nn_state.randomize()\n",
    "qr = QuantumReconstruction(nn_state)\n",
    "train_stats = ts.TrainingStatistics(train_samples.shape[-1],frequency=log_every)\n",
    "train_stats.load(bases = bases,target_psi = target_psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr.fit(train_samples, epochs, batch_size, num_chains, k,\n",
    "       lr,input_bases=train_bases, progbar=False,observer=train_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rbm_tutorial import RBM_Module, ComplexRBM\n",
    "import torch\n",
    "import cplx\n",
    "import unitary_library\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*rbm_tutorial.py* contains the child class **ComplexRBM** that inherits properties and functions from the parent class **RBM_Module**. \n",
    " \n",
    "Pytorch (torch) is used as a replacement for doing some algebra that would normally be done with numpy. Pytorch also allows one to take advantage of GPU acceleration among many other things. Don't worry if you don't have a GPU on your machine; the tutorial will run in no time on a CPU.\n",
    "\n",
    "One downfall of pytorch is that it currently does not have complex number support, so we have written our own complex algebra library (cplx.py). For more information on this library's contents, please refer to [here](../cplx.rst). We hope that pytorch will implement complex numbers soon!\n",
    "\n",
    "*unitary_library* is a package that will create a dictionary of the unitaries needed in order to train a ComplexRBM object (more later).\n",
    "\n",
    "## Training\n",
    "\n",
    "Let's go through training a complex wavefunction. To evaluate how the RBM is training, we will compute the fidelity between the true wavefunction of the system and the wavefunction the RBM reconstructs. We first need to load our training data and the true wavefunction of this system. However, we also need the corresponding file that contains all of the measurements that each site is in. The dummy dataset we will train our RBM on is a two qubit system who's wavefunction is $\\psi =\\left.\\frac{1}{2}\\right\\vert+,+\\rangle - \\left.\\frac{1}{2}\\right\\vert+,-\\rangle + \\left.\\frac{i}{2}\\right\\vert-,+\\rangle - \\left.\\frac{i}{2}\\right\\vert-,-\\rangle$, where $+$ and $-$ represent spin-up and spin-down, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set2 = np.loadtxt('2qubits_train_samples.txt', dtype= 'float32')\n",
    "psi_file   = np.loadtxt('2qubits_psi.txt')\n",
    "true_psi2  = torch.tensor([psi_file[:,0], psi_file[:,1]], dtype = torch.double)\n",
    "bases      = np.loadtxt('2qubits_train_bases.txt', dtype = str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following arguments are required to construct a **ComplexRBM** object.\n",
    "\n",
    "1. **A dictionary containing 2x2 unitaries, unitaries**. We will create this dictionary in the next block with the hand of the module we imported called *unitary_library*.\n",
    "2. **The number of visible units, num_visible**. This is 2 for the case of our dataset.\n",
    "3. **The number of hidden units in the amplitude hidden layer of the RBM, num_hidden_amp**. It's recommended that the number of hidden units stay equal to the number of visible units (2 in the case of our dummy dataset).\n",
    "4. **The number of hidden units in the phase hidden layer of the RBM, num_hidden_amp**. It's recommended that the number of hidden units stay equal to the number of visible units (2 in the case of our dummy dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unitaries = unitary_library.create_dict()\n",
    "'''If you would like to add your own quantum gates from your experiment to \"unitaries\", do:\n",
    "unitaries = unitary_library.create_dict(name='your_name', \n",
    "                                        unitary=torch.tensor([[real part], [imaginary part]], dtype=torch.double)\n",
    "For example: \n",
    "unitaries = unitary_library.create_dict(name='qucumber', unitary=torch.tensor([ [[1.,0.],[0.,1.]] \n",
    "                                                                                [[0.,0.],[0.,0.]] ], dtype=torch.double))\n",
    "                                                                                             \n",
    "By default, unitary_library.create_dict() contains the hadamard and K gates with keys X and Y, respectively.'''\n",
    "\n",
    "num_visible      = train_set2.shape[-1] # 2\n",
    "num_hidden_amp   = train_set2.shape[-1] # 2\n",
    "num_hidden_phase = train_set2.shape[-1] # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **ComplexRBM** object has a function called *fit* that performs the training. *fit* takes the following arguments.\n",
    "\n",
    "1. ***train_set***. Needed for selecting mini batches of the data.\n",
    "2. ***bases***. Needed for calculating gradients (performing the correct rotations).\n",
    "2. ***true_psi***. Only needed here to compute the fidelity.\n",
    "3. **The number of epochs, *epochs***. The number of training cycles that will be performed. 15 should be fine.\n",
    "4. **The mini batch size, *batch_size***. The number of data points that each mini batch will contain. We'll go with 10.\n",
    "5. **The number of contrastive divergence steps, *k***. One contrastive divergence step seems to be good enough in most cases.\n",
    "6. **The learning rate, *lr***. We will use a learning rate of 0.01 here.\n",
    "7. **How often you would like the program to update you during training, *log_every***. Every 10 epochs the program will print out the fidelity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs     = 15\n",
    "batch_size = 10\n",
    "k          = 1\n",
    "lr         = 0.01\n",
    "log_every  = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rbm_complex = ComplexRBM(num_visible, num_hidden_amp, num_hidden_phase)\n",
    "rbm_complex.fit(train_set2, bases, true_psi2, unitaries, epochs, batch_size, k, lr, log_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Training \n",
    "\n",
    "After training your RBM, the *fit* function will have saved your trained weights and biases for the amplitude and the phase. Now, you have the option to generate new data from the trained RBM. The *rbm_real* object has a *sample* function that takes the following arguments.\n",
    "\n",
    "1. The number of samples you wish to generate, *num_samples*.\n",
    "2. The number of contrastive divergence steps performed to generate the samples, *k*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "k           = 10\n",
    "\n",
    "samples = rbm_complex.sample(num_samples, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now find the *generated_samples_complexRBM.pkl* file in your directory that contains your new samples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
