@article{iso_scale-invariant_2018,
	title = {Scale-invariant feature extraction of neural network and renormalization group flow},
	volume = {97},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.97.053304},
	doi = {10.1103/PhysRevE.97.053304},
	abstract = {Theoretical understanding of how a deep neural network (DNN) extracts features from input images is still unclear, but it is widely believed that the extraction is performed hierarchically through a process of coarse graining. It reminds us of the basic renormalization group (RG) concept in statistical physics. In order to explore possible relations between DNN and RG, we use the restricted Boltzmann machine (RBM) applied to an Ising model and construct a flow of model parameters (in particular, temperature) generated by the RBM. We show that the unsupervised RBM trained by spin configurations at various temperatures from T=0 to T=6 generates a flow along which the temperature approaches the critical value Tc=2.27. This behavior is the opposite of the typical RG flow of the Ising model. By analyzing various properties of the weight matrices of the trained RBM, we discuss why it flows towards Tc and how the RBM learns to extract features of spin configurations.},
	number = {5},
	urldate = {2018-12-05},
	journal = {Phys. Rev. E},
	author = {Iso, Satoshi and Shiba, Shotaro and Yokoo, Sumito},
	month = may,
	year = {2018},
	pages = {053304},
	file = {APS Snapshot:/home/meach/Zotero/storage/2Z824N4P/PhysRevE.97.html:text/html;Submitted Version:/home/meach/Zotero/storage/ILWCUR3Q/Iso et al. - 2018 - Scale-invariant feature extraction of neural netwo.pdf:application/pdf}
}

@article{koch-janusz_mutual_2018,
	title = {Mutual information, neural networks and the renormalization group},
	volume = {14},
	copyright = {2018 The Author(s)},
	issn = {1745-2481},
	url = {https://www.nature.com/articles/s41567-018-0081-4},
	doi = {10.1038/s41567-018-0081-4},
	abstract = {Finding the relevant degrees of freedom of a system is a key step in any renormalization group procedure. But this can be difficult, particularly in strongly interacting systems. A machine-learning algorithm proves adept at identifying them for us.},
	language = {En},
	number = {6},
	urldate = {2018-12-05},
	journal = {Nature Physics},
	author = {Koch-Janusz, Maciej and Ringel, Zohar},
	month = jun,
	year = {2018},
	pages = {578},
	file = {Snapshot:/home/meach/Zotero/storage/UVW8V7V7/s41567-018-0081-4.html:text/html;Submitted Version:/home/meach/Zotero/storage/4BGPQHMZ/Koch-Janusz and Ringel - 2018 - Mutual information, neural networks and the renorm.pdf:application/pdf}
}

@article{li_neural_2018,
	title = {Neural {Network} {Renormalization} {Group}},
	url = {https://arxiv.org/abs/1802.02840v3},
	language = {en},
	urldate = {2018-12-05},
	author = {Li, Shuo-Hui and Wang, Lei},
	month = feb,
	year = {2018},
	file = {Full Text PDF:/home/meach/Zotero/storage/QFLVGFX8/Li and Wang - 2018 - Neural Network Renormalization Group.pdf:application/pdf;Snapshot:/home/meach/Zotero/storage/2BBUNMH5/1802.html:text/html}
}

@article{efthymiou_super-resolving_2018,
	title = {Super-resolving the {Ising} model with convolutional neural networks},
	url = {http://arxiv.org/abs/1810.02372},
	abstract = {Machine learning is becoming widely used in condensed matter physics. Inspired by the concept of image super-resolution, we propose a method to increase the size of lattice spin configurations using deep convolutional neural networks. Through supervised learning on Monte Carlo (MC) generated spin configurations, we train networks that invert real-space renormalization decimations. We demonstrate that super-resolution can reproduce thermodynamic observables that agree with MC calculations for the one and two-dimensional Ising model at various temperatures. We find that it is possible to predict thermodynamic quantities for lattice sizes larger than those used in training by extrapolating the parameters of the network. We use this method to extrapolate the exponents of the 2D Ising critical point towards the thermodynamic limit, which results in good agreement with theory.},
	urldate = {2018-12-05},
	journal = {arXiv:1810.02372 [cond-mat]},
	author = {Efthymiou, Stavros and Beach, Matthew J. S. and Melko, Roger G.},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.02372},
	keywords = {Condensed Matter - Statistical Mechanics},
	file = {arXiv\:1810.02372 PDF:/home/meach/Zotero/storage/SNC2PQ8T/Efthymiou et al. - 2018 - Super-resolving the Ising model with convolutional.pdf:application/pdf;arXiv.org Snapshot:/home/meach/Zotero/storage/AJH2D3TS/1810.html:text/html}
}

@article{beny_deep_2013,
	title = {Deep learning and the renormalization group},
	url = {http://arxiv.org/abs/1301.3124},
	abstract = {Renormalization group (RG) methods, which model the way in which the effective behavior of a system depends on the scale at which it is observed, are key to modern condensed-matter theory and particle physics. We compare the ideas behind the RG on the one hand and deep machine learning on the other, where depth and scale play a similar role. In order to illustrate this connection, we review a recent numerical method based on the RG---the multiscale entanglement renormalization ansatz (MERA)---and show how it can be converted into a learning algorithm based on a generative hierarchical Bayesian network model. Under the assumption---common in physics---that the distribution to be learned is fully characterized by local correlations, this algorithm involves only explicit evaluation of probabilities, hence doing away with sampling.},
	urldate = {2018-12-05},
	journal = {arXiv:1301.3124 [quant-ph]},
	author = {Bény, Cédric},
	month = jan,
	year = {2013},
	note = {arXiv: 1301.3124},
	keywords = {Quantum Physics},
	file = {arXiv\:1301.3124 PDF:/home/meach/Zotero/storage/XDG5IXJ9/Bény - 2013 - Deep learning and the renormalization group.pdf:application/pdf;arXiv.org Snapshot:/home/meach/Zotero/storage/XCI4Z65H/1301.html:text/html;Full Text PDF:/home/meach/Zotero/storage/I7YATXYJ/Bény - 2013 - Deep learning and the renormalization group.pdf:application/pdf;Snapshot:/home/meach/Zotero/storage/F28CVL6Q/1301.html:text/html}
}

@article{lenggenhager_optimal_2018,
	title = {Optimal {Renormalization} {Group} {Transformation} from {Information} {Theory}},
	url = {http://arxiv.org/abs/1809.09632},
	abstract = {The connections between information theory, statistical physics and quantum field theory have been the focus of renewed attention. In particular, the renormalization group (RG) has been explored from this perspective. Recently, a variational algorithm employing machine learning tools to identify the relevant degrees of freedom of a statistical system by maximizing an information-theoretic quantity, the real-space mutual information (RSMI), was proposed for real-space RG. Here we investigate analytically the RG coarse-graining procedure and the renormalized Hamiltonian, which the RSMI algorithm defines. By a combination of general arguments, exact calculations and toy models we show that the RSMI coarse-graining is optimal in a sense we define. In particular, a perfect RSMI coarse-graining generically does not increase the range of a short-ranged Hamiltonian, in any dimension. For the case of the 1D Ising model we perturbatively derive the dependence of the coefficients of the renormalized Hamiltonian on the real-space mutual information retained by a generic coarse-graining procedure. We also study the dependence of the optimal coarse-graining on the prior constraints on the number and type of coarse-grained variables. We construct toy models illustrating our findings.},
	urldate = {2018-12-05},
	author = {Lenggenhager, Patrick M. and Ringel, Zohar and Huber, Sebastian D. and Koch-Janusz, Maciej},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.09632},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics},
}