@article{Gao,
	Author = {Gao, Xun and Duan, Lu-Ming},
	Da = {2017/09/22},
	Doi = {10.1038/s41467-017-00705-2},
	Id = {Gao2017},
	Isbn = {2041-1723},
	Journal = {Nature Communications},
	Number = {1},
	Pages = {662},
	Title = {Efficient representation of quantum many-body states with deep neural networks},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/s41467-017-00705-2},
	Volume = {8},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41467-017-00705-2}}
}

@article{TorlaiMixed,
	title = {Latent Space Purification via Neural Density Operators},
	author = {Torlai, Giacomo and Melko, Roger G.},
	journal = {Phys. Rev. Lett.},
	volume = {120},
	issue = {24},
	pages = {240503},
	numpages = {5},
	year = {2018},
	month = {Jun},
	publisher = {American Physical Society},
	url = {https://link.aps.org/doi/10.1103/Phys Revet.120.240503}
}


@article{Maciej,
	Author = {Koch-Janusz, Maciej and Ringel, Zohar},
	Da = {2018/06/01},
	Date-Added = {2018-09-24 19:58:38 +0000},
	Date-Modified = {2018-09-24 19:58:38 +0000},
	Doi = {10.1038/s41567-018-0081-4},
	Id = {Koch-Janusz2018},
	Isbn = {1745-2481},
	Journal = {Nature Physics},
	Number = {6},
	Pages = {578--582},
	Title = {Mutual information, neural networks and the renormalization group},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/s41567-018-0081-4},
	Volume = {14},
	Year = {2018},
}


@article{itensor,
	note={Calculations were performed using the ITensor Library (version 0.2.4)},
	title={http://itensor.org/},
	url={http://itensor.org/}
}

@inproceedings{paszke2017automatic,
	title={Automatic differentiation in PyTorch},
	author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	booktitle={NIPS-W},
	year={2017}
}

@article{zwerger2003mott,
	Abstract = {We discuss the superfluid (SF) to Mott-insulator transition of cold atoms in optical lattices recently observed by Greiner et al (2002 Nature 415 39). The fundamental properties of both phases and their experimental signatures are discussed carefully, including the limitations of the standard Gutzwiller approximation. It is shown that in a one-dimensional dilute Bose-gas with a strong transverse confinement (Tonks-gas), even an arbitrary weak optical lattice is able to induce a Mott-like state with crystalline order, provided the dimensionless interaction parameter is larger than a critical value of order one. The SF--insulator transition of the Bose--Hubbard model in this case continuously evolves into a transition of the commensurate--incommensurate type with decreasing strength of the external optical lattice.},
	Author = {Wilhelm Zwerger},
	Date-Added = {2017-10-10 13:10:51 +0000},
	Date-Modified = {2017-10-10 13:11:17 +0000},
	Journal = {Journal of Optics B: Quantum and Semiclassical Optics},
	Number = {2},
	Pages = {S9},
	Title = {{Mott--Hubbard} transition of cold atoms in optical lattices},
	Volume = {5},
	Year = {2003},
	Bdsk-Url-1 = {http://stacks.iop.org/1464-4266/5/i=2/a=352}}

@article{Torlai2016thermo,
	title = {Learning thermodynamics with Boltzmann machines},
	author = {Torlai, Giacomo and Melko, Roger G.},
	journal = {Phys. Rev. B},
	volume = {94},
	issue = {16},
	pages = {165134},
	numpages = {7},
	year = {2016},
	month = {Oct},
	publisher = {American Physical Society},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.94.165134}
}

@article{torlai2018tomography,
	title={Neural-network quantum state tomography},
	author={Torlai, Giacomo and Mazzola, Guglielmo and Carrasquilla, Juan and Troyer, Matthias and Melko, Roger and Carleo, Giuseppe},
	journal={Nature Physics},
	volume={14},
	pages={447},
	year={2018},
	publisher={Nature Publishing Group}
}

@article {CarleoTroyer2017Science,
	author = {Carleo, Giuseppe and Troyer, Matthias},
	title = {Solving the quantum many-body problem with artificial neural networks},
	volume = {355},
	pages = {602--606},
	year = {2017},
	publisher = {American Association for the Advancement of Science},
	abstract = {Elucidating the behavior of quantum interacting systems of many particles remains one of the biggest challenges in physics. Traditional numerical methods often work well, but some of the most interesting problems leave them stumped. Carleo and Troyer harnessed the power of machine learning to develop a variational approach to the quantum many-body problem (see the Perspective by Hush). The method performed at least as well as state-of-the-art approaches, setting a benchmark for a prototypical two-dimensional problem. With further development, it may well prove a valuable piece in the quantum toolbox.Science, this issue p. 602; see also p. 580The challenge posed by the many-body problem in quantum physics originates from the difficulty of describing the nontrivial correlations encoded in the exponential complexity of the many-body wave function. Here we demonstrate that systematic machine learning of the wave function can reduce this complexity to a tractable computational form for some notable cases of physical interest. We introduce a variational representation of quantum states based on artificial neural networks with a variable number of hidden neurons. A reinforcement-learning scheme we demonstrate is capable of both finding the ground state and describing the unitary time evolution of complex interacting quantum systems. Our approach achieves high accuracy in describing prototypical interacting spins models in one and two dimensions.},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/355/6325/602},
	journal = {Science}
}

@book{Smolensky,
	author={Paul Smolensky},
	year={1986},
	chapter={6},
	title={Information Processing in Dynamical Systems: Foundations of Harmony Theory},
	pages={194–281},
	editor = {Rumelhart, David E. and McClelland, James L. and PDP Research Group, CORPORATE},
	booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations},
	isbn = {0-262-68053-X},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA}
}

@article{hinton2002training,
	title={Training products of experts by minimizing contrastive divergence},
	author={Hinton, Geoffrey E},
	journal={Neural computation},
	volume={14},
	pages={1771--1800},
	year={2002},
	publisher={MIT Press}
}

@incollection{hinton2012practical,
	title={A practical guide to training restricted Boltzmann machines},
	author={Hinton, Geoffrey E},
	booktitle={Neural networks: Tricks of the trade},
	pages={599--619},
	year={2012},
	publisher={Springer}
}

@article{GlasserCirac2018,
	title = {Neural-Network Quantum States, String-Bond States, and Chiral Topological States},
	author = {Glasser, Ivan and Pancotti, Nicola and August, Moritz and Rodriguez, Ivan D. and Cirac, J. Ignacio},
	journal = {Phys. Rev. X},
	volume = {8},
	issue = {1},
	pages = {011006},
	numpages = {16},
	year = {2018},
	month = {Jan},
	publisher = {American Physical Society},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.8.011006}
}

@article{ChenWang2018,
	title = {Equivalence of restricted Boltzmann machines and tensor network states},
	author = {Chen, Jing and Cheng, Song and Xie, Haidong and Wang, Lei and Xiang, Tao},
	journal = {Phys. Rev. B},
	volume = {97},
	issue = {8},
	pages = {085104},
	numpages = {16},
	year = {2018},
	month = {Feb},
	publisher = {American Physical Society},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.97.085104}
}

@article{Hinton02,
	author = {Hinton, Geoffrey E.},
	title = {Training Products of Experts by Minimizing Contrastive Divergence},
	journal = {Neural Computation},
	volume = {14},
	pages = {1771-1800},
	year = {2002},
	URL = {
			https://doi.org/10.1162/089976602760128018

		},
	eprint = {
			https://doi.org/10.1162/089976602760128018}}


@article{gambetta18,
	author={Nikolaj Moll and Panagiotis Barkoutsos and Lev S Bishop and Jerry M Chow and Andrew Cross and Daniel J Egger and Stefan
			Filipp and Andreas Fuhrer and Jay M Gambetta and Marc Ganzhorn and Abhinav Kandala and Antonio Mezzacapo and Peter
			Müller and Walter Riess and Gian Salis and John Smolin and Ivano Tavernelli and Kristan Temme},
	title={Quantum optimization using variational algorithms on near-term quantum devices},
	journal={Quantum Science and Technology},
	volume={3},
	pages={030503},
	url={http://stacks.iop.org/2058-9565/3/i=3/a=030503},
	year={2018},
	abstract={Universal fault-tolerant quantum computers will require error-free execution of long sequences of quantum gate operations, which is expected to involve millions of physical qubits. Before the full power of such machines will be available, near-term quantum devices will provide several hundred qubits and limited error correction. Still, there is a realistic prospect to run useful algorithms within the limited circuit depth of such devices. Particularly promising are optimization algorithms that follow a hybrid approach: the aim is to steer a highly entangled state on a quantum system to a target state that minimizes a cost function via variation of some gate parameters. This variational approach can be used both for classical optimization problems as well as for problems in quantum chemistry. The challenge is to converge to the target state given the limited coherence time and connectivity of the qubits. In this context, the quantum volume as a metric to compare the power of near-term quantum devices is discussed. With focus on chemistry applications, a general description of variational algorithms is provided and the mapping from fermions to qubits is explained. Coupled-cluster and heuristic trial wave-functions are considered for efficiently finding molecular ground states. Furthermore, simple error-mitigation schemes are introduced that could improve the accuracy of determining ground-state energies. Advancing these techniques may lead to near-term demonstrations of useful quantum computation with systems containing several hundred qubits.}
}

@article{Hinton06,
	title={A Fast Learning Algorithm for Deep Belief Nets},
	author={G. Hinton and S. Osindero and Y. Teh},
	journal={Neural computation},
	volume={18},
	pages={1527--1554},
	year={2006},
	publisher={MIT Press},
	url = "http://www.mitpressjournals.org/doi/abs/10.1162/neco.2006.18.7.1527#.VzSfdWamvEY"
}

@article {Hinton504,
	author = {Hinton, G. E. and Salakhutdinov, R. R.},
	title = {Reducing the Dimensionality of Data with Neural Networks},
	volume = {313},
	pages = {504--507},
	year = {2006},
	publisher = {American Association for the Advancement of Science},
	abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such {\textquotedblleft}autoencoder{\textquotedblright} networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/313/5786/504},
	eprint = {http://science.sciencemag.org/content/313/5786/504.full.pdf},
	journal = {Science}
}


@article{gambetta17,
	Author = {Kandala, Abhinav and Mezzacapo, Antonio and Temme, Kristan and Takita, Maika and Brink, Markus and Chow, Jerry M. and Gambetta, Jay M.},
	Date = {2017/09/13/online},
	Date-Added = {2018-09-11 19:43:16 +0000},
	Date-Modified = {2018-09-11 19:43:16 +0000},
	Day = {13},
	Journal = {Nature},
	L3 = {10.1038/nature23879; https://www.nature.com/articles/nature23879#supplementary-information},
	Month = {09},
	Pages = {242 EP  -},
	Publisher = {Macmillan Publishers Limited, part of Springer Nature. All rights reserved. SN  -},
	Title = {Hardware-efficient variational quantum eigensolver for small molecules and quantum magnets},
	Ty = {JOUR},
	Url = {http://dx.doi.org/10.1038/nature23879},
	Volume = {549},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.1038/nature23879}}

@article{PhysRevLett.119.030501,
	title = {Neural Decoder for Topological Codes},
	author = {Torlai, Giacomo and Melko, Roger G.},
	journal = {Phys. Rev. Lett.},
	volume = {119},
	issue = {3},
	pages = {030501},
	numpages = {5},
	year = {2017},
	month = {Jul},
	publisher = {American Physical Society},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.119.030501}
}



@article{Bernien17,
	Author = {Bernien, Hannes and Schwartz, Sylvain and Keesling, Alexander and Levine, Harry and Omran, Ahmed and Pichler, Hannes and Choi, Soonwon and Zibrov, Alexander S. and Endres, Manuel and Greiner, Markus and Vuleti{\'c}, Vladan and Lukin, Mikhail D.},
	Journal = {Nature},
	Month = {11},
	Pages = {579},
	Title = {Probing many-body dynamics on a 51-atom quantum simulator},
	Volume = {551},
	Year = {2017}
}

@article{PhysRevB.95.035105,
	title = {Accelerated Monte Carlo simulations with restricted Boltzmann machines},
	author = {Huang, Li and Wang, Lei},
	journal = {Phys. Rev. B},
	volume = {95},
	issue = {3},
	pages = {035105},
	numpages = {6},
	year = {2017},
	month = {Jan},
	publisher = {American Physical Society},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.95.035105}
}
@article{PhysRevB.95.041101,
	title = {Self-learning Monte Carlo method},
	author = {Liu, Junwei and Qi, Yang and Meng, Zi Yang and Fu, Liang},
	journal = {Phys. Rev. B},
	volume = {95},
	issue = {4},
	pages = {041101},
	numpages = {5},
	year = {2017},
	month = {Jan},
	publisher = {American Physical Society},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.95.041101}
}

@article{PhysRevB.96.205152,
	title = {Restricted Boltzmann machine learning for solving strongly correlated quantum systems},
	author = {Nomura, Yusuke and Darmawan, Andrew S. and Yamaji, Youhei and Imada, Masatoshi},
	journal = {Phys. Rev. B},
	volume = {96},
	issue = {20},
	pages = {205152},
	numpages = {8},
	year = {2017},
	month = {Nov},
	publisher = {American Physical Society},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.96.205152}
}

@article{Swap,
	title = {Measuring Renyi Entanglement Entropy in Quantum Monte Carlo Simulations},
	author = {Hastings, Matthew B. and Gonz\'alez, Iv\'an and Kallin, Ann B. and Melko, Roger G.},
	journal = {Phys. Rev. Lett.},
	volume = {104},
	issue = {15},
	pages = {157201},
	numpages = {4},
	year = {2010},
	month = {Apr},
	publisher = {American Physical Society},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.104.157201}
}

@article{ion53,
	author = {Zhang, J.  and Pagano, G.  and Hess, P. W.  and Kyprianidis, A.  and Becker, P.  and Kaplan, H.  and Gorshkov, A. V.  and Gong, Z.-X.  and Monroe, C.},
	title={Observation of a many-body dynamical phase transition with a 53-qubit quantum simulator},
	journal={Nature},
	volume={551},
	pages={601},
	year={2017}
}

@article{RBM_stabilizer,
	title={Restricted Boltzmann Machines and Matrix Product States of 1D Translational Invariant Stabilizer Codes},
	author={Yunqin Zheng and Huan He and Nicolas Regnault and B. Andrei Bernevig},
	eprint={arXiv:1812.08171},
	year={2018}
}

@article{RBMreview,
	author={Giacomo Torlai and Roger G. Melko},
	title={Machine learning quantum states in the nisq era},
	year={2019},
	eprint={arXiv:1905.04312}
}

@article{torlai_neural-network_2018,
	title = {Neural-network quantum state tomography},
	volume = {14},
	copyright = {2018 The Author(s)},
	issn = {1745-2481},
	url = {https://www.nature.com/articles/s41567-018-0048-5},
	abstract = {Unsupervised machine learning techniques can efficiently perform quantum state tomography of large, highly entangled states with high accuracy, and allow the reconstruction of many-body quantities from simple experimentally accessible measurements.},
	urldate = {2018-12-05},
	journal = {Nature Physics},
	author = {Torlai, Giacomo and Mazzola, Guglielmo and Carrasquilla, Juan and Troyer, Matthias and Melko, Roger and Carleo, Giuseppe},
	month = may,
	year = {2018},
	pages = {447},
}

@article{carrasquilla_reconstructing_2018,
	title = {Reconstructing quantum states with generative models},
	journal={Nature Machine Intelligence},
	author={Juan Carrasquilla and Giacomo Torlai and Roger G. Melko and Leandro Aolita},
	volume={1},
	pages={155-161},
	year={2019}
}

@article{torlai_latent_2018,
	title = {Latent {Space} {Purification} via {Neural} {Density} {Operators}},
	volume = {120},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.120.240503},
	abstract = {Machine learning is actively being explored for its potential to design, validate, and even hybridize with near-term quantum devices. A central question is whether neural networks can provide a tractable representation of a given quantum state of interest. When true, stochastic neural networks can be employed for many unsupervised tasks, including generative modeling and state tomography. However, to be applicable for real experiments, such methods must be able to encode quantum mixed states. Here, we parametrize a density matrix based on a restricted Boltzmann machine that is capable of purifying a mixed state through auxiliary degrees of freedom embedded in the latent space of its hidden units. We implement the algorithm numerically and use it to perform tomography on some typical states of entangled photons, achieving fidelities competitive with standard techniques.},
	urldate = {2018-12-05},
	journal = {Phys. Rev. Lett.},
	author = {Torlai, Giacomo and Melko, Roger G.},
	month = jun,
	year = {2018},
	pages = {240503},
}



@inproceedings{kim_mixed_2018,
	title = {Mixed {Quantum} {State} {Dynamics} {Estimation} with {Artificial} {Neural} {Network}},
	abstract = {In traditional quantum measurements, the size of tomography increases exponentially with the growth of qubit counts. In this paper, we introduce machine learning techniques such as Deep Neural Network (DNN) and Long Short Term Memory (LSTM) to analyze the Quantum Density Matrix up to 4 qubits, with no assumption of governing model},
	booktitle = {2018 {International} {Conference} on {Information} and {Communication} {Technology} {Convergence} ({ICTC})},
	author = {Kim, C. and Rhee, J. K. and Lee, W. and Ahn, J.},
	month = oct,
	year = {2018},
	keywords = {Atomic measurements, Deep Neural Network (DNN), Interpolation, Long Short Term Memory (LSTM), Machine learning, Mixed State, Neural networks, Quantum Density Matrix, Tomography, Training},
	pages = {740--747},
}

@article{zhang_observation_2017,
	title = {Observation of a many-body dynamical phase transition with a 53-qubit quantum simulator},
	volume = {551},
	copyright = {2017 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature24654},
	abstract = {A quantum simulator is a type of quantum computer that controls the interactions between quantum bits (or qubits) in a way that can be mapped to certain quantum many-body problems1,2. As it becomes possible to exert more control over larger numbers of qubits, such simulators will be able to tackle a wider range of problems, such as materials design and molecular modelling, with the ultimate limit being a universal quantum computer that can solve general classes of hard problems3. Here we use a quantum simulator composed of up to 53 qubits to study non-equilibrium dynamics in the transverse-field Ising model with long-range interactions. We observe a dynamical phase transition after a sudden change of the Hamiltonian, in a regime in which conventional statistical mechanics does not apply4. The qubits are represented by the spins of trapped ions, which can be prepared in various initial pure states. We apply a global long-range Ising interaction with controllable strength and range, and measure each individual qubit with an efficiency of nearly 99 per cent. Such high efficiency means that arbitrary many-body correlations between qubits can be measured in a single shot, enabling the dynamical phase transition to be probed directly and revealing computationally intractable features that rely on the long-range interactions and high connectivity between qubits.},
	urldate = {2018-12-19},
	journal = {Nature},
	author = {Zhang, J. and Pagano, G. and Hess, P. W. and Kyprianidis, A. and Becker, P. and Kaplan, H. and Gorshkov, A. V. and Gong, Z.-X. and Monroe, C.},
	month = nov,
	year = {2017},
	pages = {601--604},
}

@article{moll_quantum_2018,
	title = {Quantum optimization using variational algorithms on near-term quantum devices},
	volume = {3},
	issn = {2058-9565},
	url = {http://stacks.iop.org/2058-9565/3/i=3/a=030503},
	abstract = {Universal fault-tolerant quantum computers will require error-free execution of long sequences of quantum gate operations, which is expected to involve millions of physical qubits. Before the full power of such machines will be available, near-term quantum devices will provide several hundred qubits and limited error correction. Still, there is a realistic prospect to run useful algorithms within the limited circuit depth of such devices. Particularly promising are optimization algorithms that follow a hybrid approach: the aim is to steer a highly entangled state on a quantum system to a target state that minimizes a cost function via variation of some gate parameters. This variational approach can be used both for classical optimization problems as well as for problems in quantum chemistry. The challenge is to converge to the target state given the limited coherence time and connectivity of the qubits. In this context, the quantum volume as a metric to compare the power of near-term quantum devices is discussed. With focus on chemistry applications, a general description of variational algorithms is provided and the mapping from fermions to qubits is explained. Coupled-cluster and heuristic trial wave-functions are considered for efficiently finding molecular ground states. Furthermore, simple error-mitigation schemes are introduced that could improve the accuracy of determining ground-state energies. Advancing these techniques may lead to near-term demonstrations of useful quantum computation with systems containing several hundred qubits.},
	urldate = {2018-12-19},
	journal = {Quantum Sci. Technol.},
	author = {Moll, Nikolaj and Barkoutsos, Panagiotis and Bishop, Lev S. and Chow, Jerry M. and Cross, Andrew and Egger, Daniel J. and {Stefan Filipp} and Fuhrer, Andreas and Gambetta, Jay M. and Ganzhorn, Marc and Kandala, Abhinav and Mezzacapo, Antonio and {Peter Müller} and Riess, Walter and Salis, Gian and Smolin, John and Tavernelli, Ivano and Temme, Kristan},
	year = {2018},
	pages = {030503},
}

@article{bernien_probing_2017,
	title = {Probing many-body dynamics on a 51-atom quantum simulator},
	volume = {551},
	copyright = {2017 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature24622},
	abstract = {Controllable, coherent many-body systems can provide insights into the fundamental properties of quantum matter, enable the realization of new quantum phases and could ultimately lead to computational systems that outperform existing computers based on classical approaches. Here we demonstrate a method for creating controlled many-body quantum matter that combines deterministically prepared, reconfigurable arrays of individually trapped cold atoms with strong, coherent interactions enabled by excitation to Rydberg states. We realize a programmable Ising-type quantum spin model with tunable interactions and system sizes of up to 51 qubits. Within this model, we observe phase transitions into spatially ordered states that break various discrete symmetries, verify the high-fidelity preparation of these states and investigate the dynamics across the phase transition in large arrays of atoms. In particular, we observe robust many-body dynamics corresponding to persistent oscillations of the order after a rapid quantum quench that results from a sudden transition across the phase boundary. Our method provides a way of exploring many-body phenomena on a programmable quantum simulator and could enable realizations of new quantum algorithms.},
	urldate = {2018-12-19},
	journal = {Nature},
	author = {Bernien, Hannes and Schwartz, Sylvain and Keesling, Alexander and Levine, Harry and Omran, Ahmed and Pichler, Hannes and Choi, Soonwon and Zibrov, Alexander S. and Endres, Manuel and Greiner, Markus and Vuletić, Vladan and Lukin, Mikhail D.},
	month = nov,
	year = {2017},
	pages = {579--584},
}

@article{kandala_hardware-efficient_2017,
	title = {Hardware-efficient variational quantum eigensolver for small molecules and quantum magnets},
	volume = {549},
	copyright = {2017 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature23879},
	abstract = {Quantum computers can be used to address electronic-structure problems and problems in materials science and condensed matter physics that can be formulated as interacting fermionic problems, problems which stretch the limits of existing high-performance computers1. Finding exact solutions to such problems numerically has a computational cost that scales exponentially with the size of the system, and Monte Carlo methods are unsuitable owing to the fermionic sign problem. These limitations of classical computational methods have made solving even few-atom electronic-structure problems interesting for implementation using medium-sized quantum computers. Yet experimental implementations have so far been restricted to molecules involving only hydrogen and helium2,3,4,5,6,7,8. Here we demonstrate the experimental optimization of Hamiltonian problems with up to six qubits and more than one hundred Pauli terms, determining the ground-state energy for molecules of increasing size, up to BeH2. We achieve this result by using a variational quantum eigenvalue solver (eigensolver) with efficiently prepared trial states that are tailored specifically to the interactions that are available in our quantum processor, combined with a compact encoding of fermionic Hamiltonians9 and a robust stochastic optimization routine10. We demonstrate the flexibility of our approach by applying it to a problem of quantum magnetism, an antiferromagnetic Heisenberg model in an external magnetic field. In all cases, we find agreement between our experiments and numerical simulations using a model of the device with noise. Our results help to elucidate the requirements for scaling the method to larger systems and for bridging the gap between key problems in high-performance computing and their implementation on quantum hardware.},
	urldate = {2018-12-19},
	journal = {Nature},
	author = {Kandala, Abhinav and Mezzacapo, Antonio and Temme, Kristan and Takita, Maika and Brink, Markus and Chow, Jerry M. and Gambetta, Jay M.},
	month = sep,
	year = {2017},
	pages = {242--246},
}

@article{lennon_efficiently_2018,
	title = {Efficiently measuring a quantum device using machine learning},
	url = {http://arxiv.org/abs/1810.10042},
	abstract = {Scalable quantum technologies will present challenges for characterizing and tuning quantum devices. This is a time-consuming activity, and as the size of quantum systems increases, this task will become intractable without the aid of automation. We present measurements on a quantum dot device performed by a machine learning algorithm. The algorithm selects the most informative measurements to perform next using information theory and a probabilistic deep-generative model, the latter capable of generating multiple full-resolution reconstructions from scattered partial measurements. We demonstrate, for two different measurement configurations, that the algorithm outperforms standard grid scan techniques, reducing the number of measurements required by up to 4 times and the measurement time by 3.7 times. Our contribution goes beyond the use of machine learning for data search and analysis, and instead presents the use of algorithms to automate measurement. This work lays the foundation for automated control of large quantum circuits.},
	urldate = {2018-12-19},
	author = {Lennon, D. T. and Moon, H. and Camenzind, L. C. and Yu, Liuqi and Zumbühl, D. M. and Briggs, G. A. D. and Osborne, M. A. and Laird, E. A. and Ares, N.},
	month = oct,
	year = {2018},
	eprint= {arXiv:1810.10042},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Mesoscale and Nanoscale Physics, Quantum Physics},
}

@article{mehta_exact_2014,
	title = {An exact mapping between the {Variational} {Renormalization} {Group} and {Deep} {Learning}},
	url = {http://arxiv.org/abs/1410.3831},
	abstract = {Deep learning is a broad set of techniques that uses multiple layers of representation to automatically learn relevant features directly from structured data. Recently, such techniques have yielded record-breaking results on a diverse set of difficult machine learning tasks in computer vision, speech recognition, and natural language processing. Despite the enormous success of deep learning, relatively little is understood theoretically about why these techniques are so successful at feature learning and compression. Here, we show that deep learning is intimately related to one of the most important and successful techniques in theoretical physics, the renormalization group (RG). RG is an iterative coarse-graining scheme that allows for the extraction of relevant features (i.e. operators) as a physical system is examined at different length scales. We construct an exact mapping from the variational renormalization group, first introduced by Kadanoff, and deep learning architectures based on Restricted Boltzmann Machines (RBMs). We illustrate these ideas using the nearest-neighbor Ising Model in one and two-dimensions. Our results suggests that deep learning algorithms may be employing a generalized RG-like scheme to learn relevant features from data.},
	urldate = {2018-12-19},
	author = {Mehta, Pankaj and Schwab, David J.},
	month = oct,
	year = {2014},
	eprint= {arXiv:1410.3831},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Condensed Matter - Statistical Mechanics, Statistics - Machine Learning},
}

@article{weinstein_neural_2018,
	title = {Neural networks as "hidden" variable models for quantum systems},
	url = {http://arxiv.org/abs/1807.03910},
	abstract = {We successfully model the behavior of two-spin systems using neural networks known as conditional Restricted Boltzmann Machines (cRBMs). The result gives local "hidden" variable models for product states and entangled states, including the singlet state used in the EPR-Bohm experiment. Bell's theorem is circumvented because the state of the system is dependent not only on the preparation but also on the measurement setup (the detector settings). Though at first glance counterintuitive, the apparent "retrocausality" in these models has a historical precedent in the absorber theory of Wheeler and Feynman and an intuitive analog in the simple AC circuit of an electric guitar.},
	urldate = {2018-12-19},
	author = {Weinstein, Steven},
	month = jul,
	year = {2018},
	eprint= {arXiv:1807.03910},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics, Quantum Physics},
}

@article{nomura_restricted_2017,
	title = {Restricted {Boltzmann} machine learning for solving strongly correlated quantum systems},
	volume = {96},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.96.205152},
	abstract = {We develop a machine learning method to construct accurate ground-state wave functions of strongly interacting and entangled quantum spin as well as fermionic models on lattices. A restricted Boltzmann machine algorithm in the form of an artificial neural network is combined with a conventional variational Monte Carlo method with pair product (geminal) wave functions and quantum number projections. The combination allows an application of the machine learning scheme to interacting fermionic systems. The combined method substantially improves the accuracy beyond that ever achieved by each method separately, in the Heisenberg as well as Hubbard models on square lattices, thus proving its power as a highly accurate quantum many-body solver.},
	urldate = {2018-12-05},
	journal = {Phys. Rev. B},
	author = {Nomura, Yusuke and Darmawan, Andrew S. and Yamaji, Youhei and Imada, Masatoshi},
	month = nov,
	year = {2017},
	pages = {205152},
}

@article{torlai_learning_2016,
	title = {Learning thermodynamics with {Boltzmann} machines},
	volume = {94},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.94.165134},
	urldate = {2018-12-05},
	journal = {Phys. Rev. B},
	author = {Torlai, Giacomo and Melko, Roger G.},
	month = oct,
	year = {2016},
	pages = {165134},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	copyright = {2015 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	language = {en},
	urldate = {2018-12-05},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444},
}

@article{ackley_learning_1985,
	title = {A learning algorithm for boltzmann machines},
	volume = {9},
	issn = {0364-0213},
	url = {http://www.sciencedirect.com/science/article/pii/S0364021385800124},
	abstract = {The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.},
	urldate = {2018-12-05},
	journal = {Cognitive Science},
	author = {Ackley, David H. and Hinton, Geoffrey E. and Sejnowski, Terrence J.},
	month = jan,
	year = {1985},
	pages = {147--169},
}

@incollection{hinton_practical_2012,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Practical} {Guide} to {Training} {Restricted} {Boltzmann} {Machines}},
	isbn = {978-3-642-35289-8},
	url = {https://doi.org/10.1007/978-3-642-35289-8_32},
	abstract = {Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters. Over the last few years, the machine learning group at the University of Toronto has acquired considerable expertise at training RBMs and this guide is an attempt to share this expertise with other machine learning researchers.},
	language = {en},
	urldate = {2018-12-05},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}: {Second} {Edition}},
	publisher = {Springer Berlin Heidelberg},
	author = {Hinton, Geoffrey E.},
	editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
	year = {2012},
	keywords = {Hide Unit, Learning Rate, Reconstruction Error, Restrict Boltzmann Machine, Training Case},
	pages = {599--619},
}

@article{hinton_reducing_2006,
	title = {Reducing the {Dimensionality} of {Data} with {Neural} {Networks}},
	volume = {313},
	copyright = {American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/313/5786/504},
	language = {en},
	urldate = {2018-12-05},
	journal = {Science},
	author = {Hinton, G. E. and Salakhutdinov, R. R.},
	month = jul,
	year = {2006},
	pmid = {16873662},
	pages = {504--507},
}

@article{hinton_reducing_2006-1,
	title = {Reducing the {Dimensionality} of {Data} with {Neural} {Networks}},
	volume = {313},
	url = {http://science.sciencemag.org/content/313/5786/504.abstract},
	abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
	journal = {Science},
	author = {Hinton, G. E. and Salakhutdinov, R. R.},
	month = jul,
	year = {2006},
	pages = {504}
}

@article{hinton_training_2002,
	title = {Training {Products} of {Experts} by {Minimizing} {Contrastive} {Divergence}},
	volume = {14},
	issn = {0899-7667},
	url = {https://www.mitpressjournals.org/doi/10.1162/089976602760128018},
	abstract = {It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual “expert” models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called “contrastive divergence” whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.},
	urldate = {2018-12-05},
	journal = {Neural Computation},
	author = {Hinton, Geoffrey E.},
	month = aug,
	year = {2002},
	pages = {1771--1800},
}

@incollection{smolensky_information_1986,
title = {Information {Processing} in {Dynamical} {Systems}: {Foundations} of {Harmony} {Theory}},
isbn = {0-262-68053-X},
shorttitle = {Information {Processing} in {Dynamical} {Systems}},
url = {https://apps.dtic.mil/docs/citations/ADA620727},
language = {en},
urldate = {2018-12-05},
booktitle = {Parallel {Distributed} {Processing}: {Explorations} in the {Microstructure} of {Cognition}, {Volume} 1: {Foundations}},
publisher = {MIT Press},
author = {Smolensky, Paul},
month = feb,
year = {1986},
pages = {194--281},
}

@article{gao_efficient_2017,
	title = {Efficient representation of quantum many-body states with deep neural networks},
	volume = {8},
	copyright = {2017 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-017-00705-2},
	abstract = {One of the challenges in studies of quantum many-body physics is finding an efficient way to record the large system wavefunctions. Here the authors present an analysis of the capabilities of recently-proposed neural network representations for storing physically accessible quantum states.},
	urldate = {2018-12-05},
	journal = {Nature Communications},
	author = {Gao, Xun and Duan, Lu-Ming},
	month = sep,
	year = {2017},
	pages = {662},
}

@article{choo_symmetries_2018,
	title = {Symmetries and {Many}-{Body} {Excitations} with {Neural}-{Network} {Quantum} {States}},
	volume = {121},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.121.167204},
	abstract = {Artificial neural networks have been recently introduced as a general ansatz to represent many-body wave functions. In conjunction with variational Monte Carlo calculations, this ansatz has been applied to find Hamiltonian ground states and their energies. Here, we provide extensions of this method to study excited states, a central task in several many-body quantum calculations. First, we give a prescription that allows us to target eigenstates of a (nonlocal) symmetry of the Hamiltonian. Second, we give an algorithm to compute low-lying excited states without symmetries. We demonstrate our approach with both restricted Boltzmann machines and feed-forward neural networks. Results are shown for the one-dimensional spin-1/2 Heisenberg model, and for the one-dimensional Bose-Hubbard model. When comparing to exact results, we obtain good agreement for a large range of excited-states energies. Interestingly, we find that deep networks typically outperform shallow architectures for high-energy states.},
	urldate = {2018-12-05},
	journal = {Phys. Rev. Lett.},
	author = {Choo, Kenny and Carleo, Giuseppe and Regnault, Nicolas and Neupert, Titus},
	month = oct,
	year = {2018},
	pages = {167204},
}

@article{jonsson_neural-network_2018,
	title = {Neural-network states for the classical simulation of quantum computing},
	url = {http://arxiv.org/abs/1808.05232},
	abstract = {Simulating quantum algorithms with classical resources generally requires exponential resources. However, heuristic classical approaches are often very efficient in approximately simulating special circuit structures, for example with limited entanglement, or based on one-dimensional geometries. Here we introduce a classical approach to the simulation of general quantum circuits based on neural-network quantum states (NQS) representations. Considering a set of universal quantum gates, we derive rules for exactly applying single-qubit and two-qubit Z rotations to NQS, whereas we provide a learning scheme to approximate the action of Hadamard gates. Results are shown for the Hadamard and Fourier transform of entangled initial states for systems sizes and total circuit depths exceeding what can be currently simulated with state-of-the-art brute-force techniques. The overall accuracy obtained by the neural-network states based on Restricted Boltzmann machines is satisfactory, and offers a classical route to simulating highly-entangled circuits. In the test cases considered, we find that our classical simulations are comparable to quantum simulations affected by an incoherent noise level in the hardware of about \$10{\textasciicircum}\{-3\}\$ per gate.},
	urldate = {2018-12-05},
	author = {Jónsson, Bjarni and Bauer, Bela and Carleo, Giuseppe},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.05232},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Quantum Gases, Physics - Computational Physics, Quantum Physics},
}

@article{carleo_constructing_2018,
	title = {Constructing exact representations of quantum many-body systems with deep neural networks},
	volume = {9},
	copyright = {2018 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-018-07520-3},
	abstract = {Significant improvements in numerical methods for quantum systems often come from finding new ways of representing quantum states that can be optimized and simulated more efficiently. Here the authors demonstrate a method to calculate exact neural network representations of many-body ground states.},
	urldate = {2018-12-19},
	journal = {Nature Communications},
	author = {Carleo, Giuseppe and Nomura, Yusuke and Imada, Masatoshi},
	month = dec,
	year = {2018},
	keywords = {Condensed Matter - Strongly Correlated Electrons, Quantum Physics, Condensed Matter - Statistical Mechanics, Condensed Matter - Disordered Systems and Neural Networks, Physics - Computational Physics},
	pages = {5322},
}

@article{glasser_neural-network_2018,
	title = {Neural-{Network} {Quantum} {States}, {String}-{Bond} {States}, and {Chiral} {Topological} {States}},
	volume = {8},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.8.011006},
	abstract = {Neural-network quantum states have recently been introduced as an Ansatz for describing the wave function of quantum many-body systems. We show that there are strong connections between neural-network quantum states in the form of restricted Boltzmann machines and some classes of tensor-network states in arbitrary dimensions. In particular, we demonstrate that short-range restricted Boltzmann machines are entangled plaquette states, while fully connected restricted Boltzmann machines are string-bond states with a nonlocal geometry and low bond dimension. These results shed light on the underlying architecture of restricted Boltzmann machines and their efficiency at representing many-body quantum states. String-bond states also provide a generic way of enhancing the power of neural-network quantum states and a natural generalization to systems with larger local Hilbert space. We compare the advantages and drawbacks of these different classes of states and present a method to combine them together. This allows us to benefit from both the entanglement structure of tensor networks and the efficiency of neural-network quantum states into a single Ansatz capable of targeting the wave function of strongly correlated systems. While it remains a challenge to describe states with chiral topological order using traditional tensor networks, we show that, because of their nonlocal geometry, neural-network quantum states and their string-bond-state extension can describe a lattice fractional quantum Hall state exactly. In addition, we provide numerical evidence that neural-network quantum states can approximate a chiral spin liquid with better accuracy than entangled plaquette states and local string-bond states. Our results demonstrate the efficiency of neural networks to describe complex quantum wave functions and pave the way towards the use of string-bond states as a tool in more traditional machine-learning applications.},
	urldate = {2018-12-05},
	journal = {Phys. Rev. X},
	author = {Glasser, Ivan and Pancotti, Nicola and August, Moritz and Rodriguez, Ivan D. and Cirac, J. Ignacio},
	month = jan,
	year = {2018},
	pages = {011006},
}

@article{iso_scale-invariant_2018,
	title = {Scale-invariant feature extraction of neural network and renormalization group flow},
	volume = {97},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.97.053304},
	abstract = {Theoretical understanding of how a deep neural network (DNN) extracts features from input images is still unclear, but it is widely believed that the extraction is performed hierarchically through a process of coarse graining. It reminds us of the basic renormalization group (RG) concept in statistical physics. In order to explore possible relations between DNN and RG, we use the restricted Boltzmann machine (RBM) applied to an Ising model and construct a flow of model parameters (in particular, temperature) generated by the RBM. We show that the unsupervised RBM trained by spin configurations at various temperatures from T=0 to T=6 generates a flow along which the temperature approaches the critical value Tc=2.27. This behavior is the opposite of the typical RG flow of the Ising model. By analyzing various properties of the weight matrices of the trained RBM, we discuss why it flows towards Tc and how the RBM learns to extract features of spin configurations.},
	urldate = {2018-12-05},
	journal = {Phys. Rev. E},
	author = {Iso, Satoshi and Shiba, Shotaro and Yokoo, Sumito},
	month = may,
	year = {2018},
	pages = {053304},
}

@article{koch-janusz_mutual_2018,
	title = {Mutual information, neural networks and the renormalization group},
	volume = {14},
	copyright = {2018 The Author(s)},
	issn = {1745-2481},
	url = {https://www.nature.com/articles/s41567-018-0081-4},
	abstract = {Finding the relevant degrees of freedom of a system is a key step in any renormalization group procedure. But this can be difficult, particularly in strongly interacting systems. A machine-learning algorithm proves adept at identifying them for us.},
	language = {En},
	urldate = {2018-12-05},
	journal = {Nature Physics},
	author = {Koch-Janusz, Maciej and Ringel, Zohar},
	month = jun,
	year = {2018},
	pages = {578},
}

@article{li_neural_2018,
	title = {Neural {Network} {Renormalization} {Group}},
	url = {https://arxiv.org/abs/1802.02840v3},
	language = {en},
	urldate = {2018-12-05},
	author = {Li, Shuo-Hui and Wang, Lei},
	month = feb,
	year = {2018},
}

@article{efthymiou_super-resolving_2018,
	title = {Super-resolving the {Ising} model with convolutional neural networks},
	url = {http://arxiv.org/abs/1810.02372},
	urldate = {2018-12-05},
	journal = {arXiv:1810.02372 [cond-mat]},
	author = {Efthymiou, Stavros and Beach, Matthew J. S. and Melko, Roger G.},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.02372},
	keywords = {Condensed Matter - Statistical Mechanics},
}

@article{beny_deep_2013,
	title = {Deep learning and the renormalization group},
	url = {http://arxiv.org/abs/1301.3124},
	urldate = {2018-12-05},
	journal = {arXiv:1301.3124 [quant-ph]},
	author = {Bény, Cédric},
	month = jan,
	year = {2013},
	note = {arXiv: 1301.3124},
	keywords = {Quantum Physics},
}

@article{lenggenhager_optimal_2018,
	title = {Optimal {Renormalization} {Group} {Transformation} from {Information} {Theory}},
	url = {http://arxiv.org/abs/1809.09632},
	urldate = {2018-12-05},
	author = {Lenggenhager, Patrick M. and Ringel, Zohar and Huber, Sebastian D. and Koch-Janusz, Maciej},
	month = sep,
	year = {2018},
	eprint= {arXiv:1809.09632},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics},
}

@article{stoudenmire_learning_2018,
	title = {Learning relevant features of data with multi-scale tensor networks},
	volume = {3},
	issn = {2058-9565},
	url = {http://stacks.iop.org/2058-9565/3/i=3/a=034003},
	abstract = {Inspired by coarse-graining approaches used in physics, we show how similar algorithms can be adapted for data. The resulting algorithms are based on layered tree tensor networks and scale linearly with both the dimension of the input and the training set size. Computing most of the layers with an unsupervised algorithm, then optimizing just the top layer for supervised classification of the MNIST and fashion MNIST data sets gives very good results. We also discuss mixing a prior guess for supervised weights together with an unsupervised representation of the data, yielding a smaller number of features nevertheless able to give good performance.},
	language = {en},
	urldate = {2018-12-05},
	journal = {Quantum Sci. Technol.},
	author = {Stoudenmire, E. Miles},
	year = {2018},
	pages = {034003},
}

@article{huggins_towards_2018,
	title = {Towards {Quantum} {Machine} {Learning} with {Tensor} {Networks}},
	url = {https://arxiv.org/abs/1803.11537},
	language = {en},
	urldate = {2018-12-05},
	author = {Huggins, William and Patel, Piyush and Whaley, K. Birgitta and Stoudenmire, E. Miles},
	month = mar,
	year = {2018},
}

@inproceedings{stoudenmire_supervised_2016,
	title = {Supervised {Learning} with {Tensor} {Networks}},
	url = {http://papers.nips.cc/paper/6211-supervised-learning-with-tensor-networks.pdf},
	urldate = {2018-12-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Stoudenmire, Edwin and Schwab, David J},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {4799--4807},
}

@article{xu_neural_2018,
	title = {Neural network state estimation for full quantum state tomography},
	url = {https://arxiv.org/abs/1811.06654},
	language = {en},
	urldate = {2018-12-05},
	author = {Xu, Qian and Xu, Shuqi},
	month = nov,
	year = {2018},
}

@article{newman_stable_2018,
	title = {Stable {Tensor} {Neural} {Networks} for {Rapid} {Deep} {Learning}},
	url = {https://arxiv.org/abs/1811.06569v1},
	language = {en},
	urldate = {2018-12-05},
	author = {Newman, Elizabeth and Horesh, Lior and Avron, Haim and Kilmer, Misha},
	month = nov,
	year = {2018},
}

@article{chen_equivalence_2018,
	title = {Equivalence of restricted {Boltzmann} machines and tensor network states},
	volume = {97},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.97.085104},
	urldate = {2018-12-05},
	journal = {Phys. Rev. B},
	author = {Chen, Jing and Cheng, Song and Xie, Haidong and Wang, Lei and Xiang, Tao},
	month = feb,
	year = {2018},
	pages = {085104},
}
