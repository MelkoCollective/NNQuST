
@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	copyright = {2015 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	language = {en},
	number = {7553},
	urldate = {2018-12-05},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444},
	file = {Snapshot:/home/meach/Zotero/storage/UP9WKZXH/nature14539.html:text/html}
}

@article{ackley_learning_1985,
	title = {A learning algorithm for boltzmann machines},
	volume = {9},
	issn = {0364-0213},
	url = {http://www.sciencedirect.com/science/article/pii/S0364021385800124},
	doi = {10.1016/S0364-0213(85)80012-4},
	abstract = {The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.},
	number = {1},
	urldate = {2018-12-05},
	journal = {Cognitive Science},
	author = {Ackley, David H. and Hinton, Geoffrey E. and Sejnowski, Terrence J.},
	month = jan,
	year = {1985},
	pages = {147--169},
	file = {Full Text:/home/meach/Zotero/storage/SCUY2ZDX/Ackley et al. - 1985 - A learning algorithm for boltzmann machines.pdf:application/pdf;ScienceDirect Snapshot:/home/meach/Zotero/storage/899P7HFD/S0364021385800124.html:text/html}
}

@incollection{hinton_practical_2012,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Practical} {Guide} to {Training} {Restricted} {Boltzmann} {Machines}},
	isbn = {978-3-642-35289-8},
	url = {https://doi.org/10.1007/978-3-642-35289-8_32},
	abstract = {Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters. Over the last few years, the machine learning group at the University of Toronto has acquired considerable expertise at training RBMs and this guide is an attempt to share this expertise with other machine learning researchers.},
	language = {en},
	urldate = {2018-12-05},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}: {Second} {Edition}},
	publisher = {Springer Berlin Heidelberg},
	author = {Hinton, Geoffrey E.},
	editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
	year = {2012},
	doi = {10.1007/978-3-642-35289-8_32},
	keywords = {Hide Unit, Learning Rate, Reconstruction Error, Restrict Boltzmann Machine, Training Case},
	pages = {599--619},
	file = {Submitted Version:/home/meach/Zotero/storage/L5LLBFAS/Hinton - 2012 - A Practical Guide to Training Restricted Boltzmann.pdf:application/pdf}
}

@article{hinton_reducing_2006,
	title = {Reducing the {Dimensionality} of {Data} with {Neural} {Networks}},
	volume = {313},
	copyright = {American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/313/5786/504},
	doi = {10.1126/science.1127647},
	abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.
Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.
Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.},
	language = {en},
	number = {5786},
	urldate = {2018-12-05},
	journal = {Science},
	author = {Hinton, G. E. and Salakhutdinov, R. R.},
	month = jul,
	year = {2006},
	pmid = {16873662},
	pages = {504--507},
	file = {Snapshot:/home/meach/Zotero/storage/FUIRA49I/504.html:text/html}
}

@article{hinton_reducing_2006-1,
	title = {Reducing the {Dimensionality} of {Data} with {Neural} {Networks}},
	volume = {313},
	url = {http://science.sciencemag.org/content/313/5786/504.abstract},
	doi = {10.1126/science.1127647},
	abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
	number = {5786},
	journal = {Science},
	author = {Hinton, G. E. and Salakhutdinov, R. R.},
	month = jul,
	year = {2006},
	pages = {504}
}

@article{hinton_training_2002,
	title = {Training {Products} of {Experts} by {Minimizing} {Contrastive} {Divergence}},
	volume = {14},
	issn = {0899-7667},
	url = {https://www.mitpressjournals.org/doi/10.1162/089976602760128018},
	doi = {10.1162/089976602760128018},
	abstract = {It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual “expert” models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called “contrastive divergence” whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.},
	number = {8},
	urldate = {2018-12-05},
	journal = {Neural Computation},
	author = {Hinton, Geoffrey E.},
	month = aug,
	year = {2002},
	pages = {1771--1800},
	file = {Snapshot:/home/meach/Zotero/storage/9A8PYN3M/089976602760128018.html:text/html;Submitted Version:/home/meach/Zotero/storage/Q8WGSXSI/Hinton - 2002 - Training Products of Experts by Minimizing Contras.pdf:application/pdf}
}

@incollection{smolensky_information_1986,
	title = {Information {Processing} in {Dynamical} {Systems}: {Foundations} of {Harmony} {Theory}},
	isbn = {0-262-68053-X},
	shorttitle = {Information {Processing} in {Dynamical} {Systems}},
	url = {https://apps.dtic.mil/docs/citations/ADA620727},
	language = {en},
	urldate = {2018-12-05},
	booktitle = {Parallel {Distributed} {Processing}: {Explorations} in the {Microstructure} of {Cognition}, {Volume} 1: {Foundations}},
	publisher = {MIT Press},
	author = {Smolensky, Paul},
	month = feb,
	year = {1986},
	pages = {194--281},
	file = {Full Text PDF:/home/meach/Zotero/storage/YS2GBH6T/Smolensky - 1986 - Information Processing in Dynamical Systems Found.pdf:application/pdf;Snapshot:/home/meach/Zotero/storage/WMIIQNUD/ADA620727.html:text/html}
}